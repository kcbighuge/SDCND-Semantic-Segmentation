{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kitti-semantic-segmentation.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": [
        "QGAjIIN2cQsu",
        "dQ-rVhIyxFl9",
        "C2b2-0f-cUbY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "7Wfid0gtSPH0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Semantic Segmentation\n",
        "\n",
        "https://github.com/kcbighuge/SDCND-Semantic-Segmentation\n"
      ]
    },
    {
      "metadata": {
        "id": "eNK9L9PaTGK8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download files"
      ]
    },
    {
      "metadata": {
        "id": "gGjE84DPSgHD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P4HroAs0SZH9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 4
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "62833e80-0f12-4eb8-b52b-d3baa9bf0c3b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521065635275,
          "user_tz": 420,
          "elapsed": 1194,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kcbighuge/SDCND-Semantic-Segmentation.git\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SDCND-Semantic-Segmentation'...\n",
            "remote: Counting objects: 54, done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 54 (delta 22), reused 52 (delta 20), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (54/54), done.\n",
            "datalab  SDCND-Semantic-Segmentation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BhtTyCLtsJGx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4612d85a-34d0-49c3-b8b7-930300cb9c98",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521068735761,
          "user_tz": 420,
          "elapsed": 574,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "\n",
        "os.chdir('SDCND-Semantic-Segmentation')\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t\t\t\t   main.py\t     runs\r\n",
            "helper.py\t\t\t   project_tests.py  sample-segmentation.png\r\n",
            "kitti_semantic_segmentation.ipynb  __pycache__\r\n",
            "LICENSE\t\t\t\t   README.md\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NoAclbMNR6h6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b410eed1-bbc9-4cb4-afae-faceae5ae549",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521065696782,
          "user_tz": 420,
          "elapsed": 52993,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "urlretrieve(\"http://kitti.is.tue.mpg.de/kitti/data_road.zip\",\"./data_road.zip\")\n",
        "with ZipFile(\"./data_road.zip\") as zipf:\n",
        "  zipf.extractall(\"./data/\")\n",
        "\n",
        "!rm data_road.zip\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t\t\t\t   LICENSE\t     README.md\r\n",
            "helper.py\t\t\t   main.py\t     sample-segmentation.png\r\n",
            "kitti_semantic_segmentation.ipynb  project_tests.py\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QGAjIIN2cQsu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install libraries\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PSCZwqjFr7Zn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 6
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "d40d0288-032d-4f94-bfb1-e0be2bcd3447",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521066312768,
          "user_tz": 420,
          "elapsed": 4450,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "!pip install --upgrade tensorflow-gpu==1.4"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already up-to-date: tensorflow-gpu==1.4 in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already up-to-date: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.4)\n",
            "Requirement already up-to-date: protobuf>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.4)\n",
            "Requirement already up-to-date: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.4)\n",
            "Requirement already up-to-date: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.4)\n",
            "Requirement already up-to-date: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.4)\n",
            "Requirement already up-to-date: enum34>=1.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.4)\n",
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.3.0->tensorflow-gpu==1.4)\n",
            "Requirement already up-to-date: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow-gpu==1.4)\n",
            "Requirement already up-to-date: bleach==1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow-gpu==1.4)\n",
            "Requirement already up-to-date: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow-gpu==1.4)\n",
            "Requirement already up-to-date: html5lib==0.9999999 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow-gpu==1.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0CFLi9VCyfkO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Upload files from desktop if necessary\n",
        "Make changes to training regime with `main.py` file. "
      ]
    },
    {
      "metadata": {
        "id": "9FCkBwy5VQqs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a73bd7c6-07a1-4f3b-a578-a1be595273f0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521066281906,
          "user_tz": 420,
          "elapsed": 754,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#os.rename('main (1).py', 'main.py')  # change name of dupe file\n",
        "!rm main.py\n",
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'main.py': No such file or directory\n",
            "data\t\t\t\t   LICENSE\t     README.md\n",
            "helper.py\t\t\t   project_tests.py  sample-segmentation.png\n",
            "kitti_semantic_segmentation.ipynb  __pycache__\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jOEgRtj7yeC9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ],
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "41a85b24-10b6-4f61-b637-f90f6617c238",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521066295725,
          "user_tz": 420,
          "elapsed": 5256,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Working with files\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dff0af70-1b41-4bca-9d7d-96a7d8ed6d21\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-dff0af70-1b41-4bca-9d7d-96a7d8ed6d21\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving main.py to main.py\n",
            "User uploaded file \"main.py\" with length 9640 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KS3P8q-qzlgg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "12266758-c97e-49fc-95b8-f248d178f074",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521068748914,
          "user_tz": 420,
          "elapsed": 522,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t\t\t\t   main.py\t     runs\r\n",
            "helper.py\t\t\t   project_tests.py  sample-segmentation.png\r\n",
            "kitti_semantic_segmentation.ipynb  __pycache__\r\n",
            "LICENSE\t\t\t\t   README.md\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dQ-rVhIyxFl9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run the project\n",
        "Use the following command to run the project\n",
        "```\n",
        "python main.py\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "kJV4mXTTxHJQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 33
            },
            {
              "item_id": 108
            },
            {
              "item_id": 182
            },
            {
              "item_id": 209
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 7313
        },
        "outputId": "68f17463-f6d5-4c9d-887e-63b28877ca73",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521070021742,
          "user_tz": 420,
          "elapsed": 1269249,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n",
            "  return f(*args, **kwds)\n",
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "TensorFlow Version: 1.4.0\n",
            "2018-03-14 23:05:54.020158: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "2018-03-14 23:05:54.144862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2018-03-14 23:05:54.145417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.11GiB\n",
            "2018-03-14 23:05:54.145445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2018-03-14 23:05:54.315365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Default GPU Device: /device:GPU:0\n",
            "2018-03-14 23:05:54.316473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Tests Passed\n",
            "Tests Passed\n",
            "2018-03-14 23:05:54.969129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Tests Passed\n",
            "2018-03-14 23:05:55.230594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Tests Passed\n",
            "Tests Passed\n",
            "2018-03-14 23:05:55.240660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if issubdtype(ts, int):\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  elif issubdtype(type(size), float):\n",
            "Epoch 1/20. Training loss: 1.0724\n",
            "\n",
            "Epoch 1/20. Training loss: 1.0532\n",
            "\n",
            "Epoch 1/20. Training loss: 1.0346\n",
            "\n",
            "Epoch 1/20. Training loss: 1.0169\n",
            "\n",
            "Epoch 1/20. Training loss: 1.0003\n",
            "\n",
            "Epoch 1/20. Training loss: 0.9846\n",
            "\n",
            "Epoch 1/20. Training loss: 0.9700\n",
            "\n",
            "Epoch 1/20. Training loss: 0.9551\n",
            "\n",
            "Epoch 1/20. Training loss: 0.9342\n",
            "\n",
            "Epoch 1/20. Training loss: 0.8392\n",
            "\n",
            "Epoch 2/20. Training loss: 0.8300\n",
            "\n",
            "Epoch 2/20. Training loss: 0.7765\n",
            "\n",
            "Epoch 2/20. Training loss: 0.6782\n",
            "\n",
            "Epoch 2/20. Training loss: 0.5717\n",
            "\n",
            "Epoch 2/20. Training loss: 0.5979\n",
            "\n",
            "Epoch 2/20. Training loss: 0.4470\n",
            "\n",
            "Epoch 2/20. Training loss: 0.4482\n",
            "\n",
            "Epoch 2/20. Training loss: 0.3942\n",
            "\n",
            "Epoch 2/20. Training loss: 0.4123\n",
            "\n",
            "Epoch 2/20. Training loss: 0.3426\n",
            "\n",
            "Epoch 3/20. Training loss: 0.4484\n",
            "\n",
            "Epoch 3/20. Training loss: 0.3670\n",
            "\n",
            "Epoch 3/20. Training loss: 0.3397\n",
            "\n",
            "Epoch 3/20. Training loss: 0.3338\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/20. Training loss: 0.3333\r\n",
            "\n",
            "Epoch 3/20. Training loss: 0.3639\n",
            "\n",
            "Epoch 3/20. Training loss: 0.3337\n",
            "\n",
            "Epoch 3/20. Training loss: 0.3145\n",
            "\n",
            "Epoch 3/20. Training loss: 0.3180\n",
            "\n",
            "Epoch 3/20. Training loss: 0.3315\n",
            "\n",
            "Epoch 4/20. Training loss: 0.2957\n",
            "\n",
            "Epoch 4/20. Training loss: 0.2957\n",
            "\n",
            "Epoch 4/20. Training loss: 0.3272\n",
            "\n",
            "Epoch 4/20. Training loss: 0.2777\n",
            "\n",
            "Epoch 4/20. Training loss: 0.2972\n",
            "\n",
            "Epoch 4/20. Training loss: 0.3154\n",
            "\n",
            "Epoch 4/20. Training loss: 0.3010\n",
            "\n",
            "Epoch 4/20. Training loss: 0.2961\n",
            "\n",
            "Epoch 4/20. Training loss: 0.3081\n",
            "\n",
            "Epoch 4/20. Training loss: 0.2167\n",
            "\n",
            "Epoch 5/20. Training loss: 0.3149\n",
            "\n",
            "Epoch 5/20. Training loss: 0.2699\n",
            "\n",
            "Epoch 5/20. Training loss: 0.2910\n",
            "\n",
            "Epoch 5/20. Training loss: 0.2598\n",
            "\n",
            "Epoch 5/20. Training loss: 0.2911\n",
            "\n",
            "Epoch 5/20. Training loss: 0.2940\n",
            "\n",
            "Epoch 5/20. Training loss: 0.2652\n",
            "\n",
            "Epoch 5/20. Training loss: 0.3044\n",
            "\n",
            "Epoch 5/20. Training loss: 0.2321\n",
            "\n",
            "Epoch 5/20. Training loss: 0.2836\n",
            "\n",
            "Epoch 6/20. Training loss: 0.2669\n",
            "\n",
            "Epoch 6/20. Training loss: 0.2949\n",
            "\n",
            "Epoch 6/20. Training loss: 0.2646\n",
            "\n",
            "Epoch 6/20. Training loss: 0.2541\n",
            "\n",
            "Epoch 6/20. Training loss: 0.2536\n",
            "\n",
            "Epoch 6/20. Training loss: 0.2193\n",
            "\n",
            "Epoch 6/20. Training loss: 0.2426\n",
            "\n",
            "Epoch 6/20. Training loss: 0.2759\n",
            "\n",
            "Epoch 6/20. Training loss: 0.2744\n",
            "\n",
            "Epoch 6/20. Training loss: 0.2223\n",
            "\n",
            "Epoch 7/20. Training loss: 0.2672\n",
            "\n",
            "Epoch 7/20. Training loss: 0.2511\n",
            "\n",
            "Epoch 7/20. Training loss: 0.2376\n",
            "\n",
            "Epoch 7/20. Training loss: 0.2439\n",
            "\n",
            "Epoch 7/20. Training loss: 0.2502\n",
            "\n",
            "Epoch 7/20. Training loss: 0.2849\n",
            "\n",
            "Epoch 7/20. Training loss: 0.2640\n",
            "\n",
            "Epoch 7/20. Training loss: 0.2569\n",
            "\n",
            "Epoch 7/20. Training loss: 0.2727\n",
            "\n",
            "Epoch 7/20. Training loss: 0.2958\n",
            "\n",
            "Epoch 8/20. Training loss: 0.2142\n",
            "\n",
            "Epoch 8/20. Training loss: 0.2596\n",
            "\n",
            "Epoch 8/20. Training loss: 0.2554\n",
            "\n",
            "Epoch 8/20. Training loss: 0.2112\n",
            "\n",
            "Epoch 8/20. Training loss: 0.2171\n",
            "\n",
            "Epoch 8/20. Training loss: 0.2168\n",
            "\n",
            "Epoch 8/20. Training loss: 0.2187\n",
            "\n",
            "Epoch 8/20. Training loss: 0.2003\n",
            "\n",
            "Epoch 8/20. Training loss: 0.2060\n",
            "\n",
            "Epoch 8/20. Training loss: 0.1971\n",
            "\n",
            "Epoch 9/20. Training loss: 0.1996\n",
            "\n",
            "Epoch 9/20. Training loss: 0.2162\n",
            "\n",
            "Epoch 9/20. Training loss: 0.2291\n",
            "\n",
            "Epoch 9/20. Training loss: 0.2609\n",
            "\n",
            "Epoch 9/20. Training loss: 0.2251\n",
            "\n",
            "Epoch 9/20. Training loss: 0.2427\n",
            "\n",
            "Epoch 9/20. Training loss: 0.2196\n",
            "\n",
            "Epoch 9/20. Training loss: 0.1776\n",
            "\n",
            "Epoch 9/20. Training loss: 0.2011\n",
            "\n",
            "Epoch 9/20. Training loss: 0.2065\n",
            "\n",
            "Epoch 10/20. Training loss: 0.2232\n",
            "\n",
            "Epoch 10/20. Training loss: 0.2450\n",
            "\n",
            "Epoch 10/20. Training loss: 0.1902\n",
            "\n",
            "Epoch 10/20. Training loss: 0.2224\n",
            "\n",
            "Epoch 10/20. Training loss: 0.2043\n",
            "\n",
            "Epoch 10/20. Training loss: 0.2295\n",
            "\n",
            "Epoch 10/20. Training loss: 0.2614\n",
            "\n",
            "Epoch 10/20. Training loss: 0.2120\n",
            "\n",
            "Epoch 10/20. Training loss: 0.2004\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/20. Training loss: 0.4464\r\n",
            "\n",
            "Epoch 11/20. Training loss: 0.2067\n",
            "\n",
            "Epoch 11/20. Training loss: 0.2390\n",
            "\n",
            "Epoch 11/20. Training loss: 0.2112\n",
            "\n",
            "Epoch 11/20. Training loss: 0.2421\n",
            "\n",
            "Epoch 11/20. Training loss: 0.2236\n",
            "\n",
            "Epoch 11/20. Training loss: 0.2120\n",
            "\n",
            "Epoch 11/20. Training loss: 0.1965\n",
            "\n",
            "Epoch 11/20. Training loss: 0.2375\n",
            "\n",
            "Epoch 11/20. Training loss: 0.2114\n",
            "\n",
            "Epoch 11/20. Training loss: 0.1818\n",
            "\n",
            "Epoch 12/20. Training loss: 0.1921\n",
            "\n",
            "Epoch 12/20. Training loss: 0.2095\n",
            "\n",
            "Epoch 12/20. Training loss: 0.2281\n",
            "\n",
            "Epoch 12/20. Training loss: 0.2289\n",
            "\n",
            "Epoch 12/20. Training loss: 0.1968\n",
            "\n",
            "Epoch 12/20. Training loss: 0.2500\n",
            "\n",
            "Epoch 12/20. Training loss: 0.1999\n",
            "\n",
            "Epoch 12/20. Training loss: 0.1711\n",
            "\n",
            "Epoch 12/20. Training loss: 0.2047\n",
            "\n",
            "Epoch 12/20. Training loss: 0.1876\n",
            "\n",
            "Epoch 13/20. Training loss: 0.2255\n",
            "\n",
            "Epoch 13/20. Training loss: 0.1878\n",
            "\n",
            "Epoch 13/20. Training loss: 0.1820\n",
            "\n",
            "Epoch 13/20. Training loss: 0.1960\n",
            "\n",
            "Epoch 13/20. Training loss: 0.1963\n",
            "\n",
            "Epoch 13/20. Training loss: 0.1951\n",
            "\n",
            "Epoch 13/20. Training loss: 0.2043\n",
            "\n",
            "Epoch 13/20. Training loss: 0.1840\n",
            "\n",
            "Epoch 13/20. Training loss: 0.1802\n",
            "\n",
            "Epoch 13/20. Training loss: 0.1846\n",
            "\n",
            "Epoch 14/20. Training loss: 0.1703\n",
            "\n",
            "Epoch 14/20. Training loss: 0.1943\n",
            "\n",
            "Epoch 14/20. Training loss: 0.2195\n",
            "\n",
            "Epoch 14/20. Training loss: 0.2056\n",
            "\n",
            "Epoch 14/20. Training loss: 0.2035\n",
            "\n",
            "Epoch 14/20. Training loss: 0.1899\n",
            "\n",
            "Epoch 14/20. Training loss: 0.1714\n",
            "\n",
            "Epoch 14/20. Training loss: 0.2087\n",
            "\n",
            "Epoch 14/20. Training loss: 0.2077\n",
            "\n",
            "Epoch 14/20. Training loss: 0.1646\n",
            "\n",
            "Epoch 15/20. Training loss: 0.1813\n",
            "\n",
            "Epoch 15/20. Training loss: 0.1918\n",
            "\n",
            "Epoch 15/20. Training loss: 0.1884\n",
            "\n",
            "Epoch 15/20. Training loss: 0.2058\n",
            "\n",
            "Epoch 15/20. Training loss: 0.1920\n",
            "\n",
            "Epoch 15/20. Training loss: 0.1755\n",
            "\n",
            "Epoch 15/20. Training loss: 0.1868\n",
            "\n",
            "Epoch 15/20. Training loss: 0.2045\n",
            "\n",
            "Epoch 15/20. Training loss: 0.1794\n",
            "\n",
            "Epoch 15/20. Training loss: 0.2001\n",
            "\n",
            "Epoch 16/20. Training loss: 0.1778\n",
            "\n",
            "Epoch 16/20. Training loss: 0.1705\n",
            "\n",
            "Epoch 16/20. Training loss: 0.2154\n",
            "\n",
            "Epoch 16/20. Training loss: 0.1951\n",
            "\n",
            "Epoch 16/20. Training loss: 0.1815\n",
            "\n",
            "Epoch 16/20. Training loss: 0.1853\n",
            "\n",
            "Epoch 16/20. Training loss: 0.1602\n",
            "\n",
            "Epoch 16/20. Training loss: 0.1906\n",
            "\n",
            "Epoch 16/20. Training loss: 0.1856\n",
            "\n",
            "Epoch 16/20. Training loss: 0.1520\n",
            "\n",
            "Epoch 17/20. Training loss: 0.1778\n",
            "\n",
            "Epoch 17/20. Training loss: 0.1702\n",
            "\n",
            "Epoch 17/20. Training loss: 0.1573\n",
            "\n",
            "Epoch 17/20. Training loss: 0.1732\n",
            "\n",
            "Epoch 17/20. Training loss: 0.1817\n",
            "\n",
            "Epoch 17/20. Training loss: 0.1656\n",
            "\n",
            "Epoch 17/20. Training loss: 0.1835\n",
            "\n",
            "Epoch 17/20. Training loss: 0.2340\n",
            "\n",
            "Epoch 17/20. Training loss: 0.2178\n",
            "\n",
            "Epoch 17/20. Training loss: 0.2828\n",
            "\n",
            "Epoch 18/20. Training loss: 0.1762\n",
            "\n",
            "Epoch 18/20. Training loss: 0.2140\n",
            "\n",
            "Epoch 18/20. Training loss: 0.1517\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/20. Training loss: 0.1842\r\n",
            "\n",
            "Epoch 18/20. Training loss: 0.1806\n",
            "\n",
            "Epoch 18/20. Training loss: 0.1766\n",
            "\n",
            "Epoch 18/20. Training loss: 0.1797\n",
            "\n",
            "Epoch 18/20. Training loss: 0.1508\n",
            "\n",
            "Epoch 18/20. Training loss: 0.2007\n",
            "\n",
            "Epoch 18/20. Training loss: 0.1611\n",
            "\n",
            "Epoch 19/20. Training loss: 0.1633\n",
            "\n",
            "Epoch 19/20. Training loss: 0.1820\n",
            "\n",
            "Epoch 19/20. Training loss: 0.1614\n",
            "\n",
            "Epoch 19/20. Training loss: 0.1613\n",
            "\n",
            "Epoch 19/20. Training loss: 0.1794\n",
            "\n",
            "Epoch 19/20. Training loss: 0.1752\n",
            "\n",
            "Epoch 19/20. Training loss: 0.1658\n",
            "\n",
            "Epoch 19/20. Training loss: 0.1665\n",
            "\n",
            "Epoch 19/20. Training loss: 0.1762\n",
            "\n",
            "Epoch 19/20. Training loss: 0.1630\n",
            "\n",
            "Epoch 20/20. Training loss: 0.1645\n",
            "\n",
            "Epoch 20/20. Training loss: 0.1681\n",
            "\n",
            "Epoch 20/20. Training loss: 0.1674\n",
            "\n",
            "Epoch 20/20. Training loss: 0.1802\n",
            "\n",
            "Epoch 20/20. Training loss: 0.1545\n",
            "\n",
            "Epoch 20/20. Training loss: 0.1725\n",
            "\n",
            "Epoch 20/20. Training loss: 0.1614\n",
            "\n",
            "Epoch 20/20. Training loss: 0.1549\n",
            "\n",
            "Epoch 20/20. Training loss: 0.1510\n",
            "\n",
            "Epoch 20/20. Training loss: 0.1711\n",
            "\n",
            "Training Finished. Saving test images to: ./runs/1521069943.1282473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w2yyEy4P1YF_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download files"
      ]
    },
    {
      "metadata": {
        "id": "EuhpG6FJ6h08",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e62e16b3-5e8b-4b12-9736-8ff13377ce81",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521070038017,
          "user_tz": 420,
          "elapsed": 2115,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive('runs_img', 'zip', './runs/1521069943.1282473')\n",
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t\t\t\t   main.py\t     runs\r\n",
            "helper.py\t\t\t   project_tests.py  runs_img.zip\r\n",
            "kitti_semantic_segmentation.ipynb  __pycache__\t     sample-segmentation.png\r\n",
            "LICENSE\t\t\t\t   README.md\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0zFh4vTY5HVY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('runs_img.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C2b2-0f-cUbY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building the model"
      ]
    },
    {
      "metadata": {
        "id": "aNOWaSdIcPnW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d95dfd41-d885-4b0f-ad1c-393bd6155dda",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521065995004,
          "user_tz": 420,
          "elapsed": 7111,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import tensorflow as tf\n",
        "import helper\n",
        "import warnings\n",
        "from distutils.version import LooseVersion\n",
        "import project_tests as tests\n",
        "\n",
        "\n",
        "# Check TensorFlow Version\n",
        "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))\n",
        "\n",
        "# Check for a GPU\n",
        "if not tf.test.gpu_device_name():\n",
        "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
        "else:\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
            "  return f(*args, **kwds)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 1.4.0\n",
            "Default GPU Device: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9NWG04n5cnXC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "88b06445-fd68-4a07-9019-203a51d946b6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521066075673,
          "user_tz": 420,
          "elapsed": 1893,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def load_vgg(sess, vgg_path):\n",
        "    \"\"\"\n",
        "    Load Pretrained VGG Model into TensorFlow.\n",
        "    :param sess: TensorFlow Session\n",
        "    :param vgg_path: Path to vgg folder, containing \"variables/\" and \"saved_model.pb\"\n",
        "    :return: Tuple of Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)\n",
        "    \"\"\"\n",
        "    # DONE: Implement function\n",
        "    #  Use tf.saved_model.loader.load to load the model and weights\n",
        "    vgg_tag = 'vgg16'\n",
        "    vgg_input_tensor_name = 'image_input:0'\n",
        "    vgg_keep_prob_tensor_name = 'keep_prob:0'\n",
        "    vgg_layer3_out_tensor_name = 'layer3_out:0'\n",
        "    vgg_layer4_out_tensor_name = 'layer4_out:0'\n",
        "    vgg_layer7_out_tensor_name = 'layer7_out:0'\n",
        "\n",
        "    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)\n",
        "    graph = tf.get_default_graph()\n",
        "    \n",
        "    image_input = graph.get_tensor_by_name(vgg_input_tensor_name)\n",
        "    keep_prob = graph.get_tensor_by_name(vgg_keep_prob_tensor_name)\n",
        "\n",
        "    layer3_out = graph.get_tensor_by_name(vgg_layer3_out_tensor_name)\n",
        "\n",
        "    layer4_out = graph.get_tensor_by_name(vgg_layer4_out_tensor_name)\n",
        "\n",
        "    layer7_out = graph.get_tensor_by_name(vgg_layer7_out_tensor_name)\n",
        "\n",
        "    return image_input, keep_prob, layer3_out, layer4_out, layer7_out\n",
        "\n",
        "tests.test_load_vgg(load_vgg, tf)\n",
        "\n",
        "\n",
        "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n",
        "    \"\"\"\n",
        "    Create the layers for a fully convolutional network.  Build skip-layers using the vgg layers.\n",
        "    :param vgg_layer3_out: TF Tensor for VGG Layer 3 output\n",
        "    :param vgg_layer4_out: TF Tensor for VGG Layer 4 output\n",
        "    :param vgg_layer7_out: TF Tensor for VGG Layer 7 output\n",
        "    :param num_classes: Number of classes to classify\n",
        "    :return: The Tensor for the last layer of output\n",
        "    \"\"\"\n",
        "    # DONE: Implement function\n",
        "    layer7_1x1 = tf.layers.conv2d(vgg_layer7_out, num_classes, 1, \n",
        "                                  padding='same', \n",
        "                                  kernel_initializer=tf.truncated_normal_initializer(stddev=.01),\n",
        "                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n",
        "    \n",
        "    output = tf.layers.conv2d_transpose(layer7_1x1, num_classes, 4, \n",
        "                                        strides=(2,2), padding='same', \n",
        "                                        kernel_initializer=tf.truncated_normal_initializer(stddev=.01),\n",
        "                                        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n",
        "    \n",
        "    # scale output of layers 3 & 4 before feeding to 1x1 convolution\n",
        "    pool3_out_scaled = tf.multiply(vgg_layer3_out, 0.0001, name='pool3_out_scaled')\n",
        "    pool4_out_scaled = tf.multiply(vgg_layer4_out, 0.01, name='pool4_out_scaled')\n",
        "\n",
        "    layer4_1x1 = tf.layers.conv2d(pool4_out_scaled, num_classes, 1, \n",
        "                                  padding='same', \n",
        "                                  kernel_initializer=tf.truncated_normal_initializer(stddev=.01),\n",
        "                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n",
        "    \n",
        "    output = tf.add(output, layer4_1x1)\n",
        "    output = tf.layers.conv2d_transpose(output, num_classes, 4, \n",
        "                                        strides=(2,2), \n",
        "                                        padding='same', \n",
        "                                        kernel_initializer=tf.truncated_normal_initializer(stddev=.01),\n",
        "                                        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n",
        "    \n",
        "    layer3_1x1 = tf.layers.conv2d(pool3_out_scaled, num_classes, 1, \n",
        "                                  padding='same', \n",
        "                                  kernel_initializer=tf.truncated_normal_initializer(stddev=.01),\n",
        "                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n",
        "    \n",
        "    output = tf.add(output, layer3_1x1)\n",
        "    output = tf.layers.conv2d_transpose(output, num_classes, 16, \n",
        "                                        strides=(8,8), \n",
        "                                        padding='same', \n",
        "                                        kernel_initializer=tf.truncated_normal_initializer(stddev=.01),\n",
        "                                        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n",
        "\n",
        "    return output\n",
        "\n",
        "tests.test_layers(layers)\n",
        "\n",
        "\n",
        "def optimize(nn_last_layer, correct_label, learning_rate, num_classes):\n",
        "    \"\"\"\n",
        "    Build the TensorFLow loss and optimizer operations.\n",
        "    :param nn_last_layer: TF Tensor of the last layer in the neural network\n",
        "    :param correct_label: TF Placeholder for the correct label image\n",
        "    :param learning_rate: TF Placeholder for the learning rate\n",
        "    :param num_classes: Number of classes to classify\n",
        "    :return: Tuple of (logits, train_op, cross_entropy_loss)\n",
        "    \"\"\"\n",
        "    # DONE: Implement function\n",
        "    # Convert 4D output tensor to 2D with shape (num pixels, num classes)\n",
        "    logits = tf.reshape(nn_last_layer, (-1, num_classes))\n",
        "    labels = tf.reshape(correct_label, (-1, num_classes))\n",
        "\n",
        "    # Define loss\n",
        "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
        "    cross_entropy_loss = tf.reduce_mean(cross_entropy)\n",
        "\n",
        "    # Need to include additional term for regularization loss\n",
        "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "    cross_entropy_loss = cross_entropy_loss + sum(reg_losses)\n",
        "\n",
        "    # Define optimization\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy_loss)\n",
        "\n",
        "    return logits, train_op, cross_entropy_loss\n",
        "\n",
        "tests.test_optimize(optimize)\n",
        "\n",
        "\n",
        "def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image,\n",
        "             correct_label, keep_prob, learning_rate):\n",
        "    \"\"\"\n",
        "    Train neural network and print out the loss during training.\n",
        "    :param sess: TF Session\n",
        "    :param epochs: Number of epochs\n",
        "    :param batch_size: Batch size\n",
        "    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n",
        "    :param train_op: TF Operation to train the neural network\n",
        "    :param cross_entropy_loss: TF Tensor for the amount of loss\n",
        "    :param input_image: TF Placeholder for input images\n",
        "    :param correct_label: TF Placeholder for label images\n",
        "    :param keep_prob: TF Placeholder for dropout keep probability\n",
        "    :param learning_rate: TF Placeholder for learning rate\n",
        "    \"\"\"\n",
        "    # DONE: Implement function\n",
        "    for e in range(epochs):\n",
        "\n",
        "        i = 0\n",
        "        for images, labels in get_batches_fn(batch_size):\n",
        "            _, loss = sess.run([train_op, cross_entropy_loss], \n",
        "                               feed_dict={input_image: images, \n",
        "                                          correct_label: labels, \n",
        "                                          keep_prob: 0.5, \n",
        "                                          learning_rate: 1e-4})\n",
        "            if i % 4 == 0:\n",
        "                print(\"Epoch {}/{}. Training loss: {:.4f}\\n\".format(e+1, epochs, loss))\n",
        "            i += 1\n",
        "\n",
        "tests.test_train_nn(train_nn)\n",
        "\n",
        "\n",
        "def run():\n",
        "    num_classes = 2\n",
        "    image_shape = (160, 576)\n",
        "    # Download data: http://kitti.is.tue.mpg.de/kitti/data_road.zip\n",
        "    data_dir = './data'\n",
        "    runs_dir = './runs'\n",
        "    tests.test_for_kitti_dataset(data_dir)\n",
        "\n",
        "    # Download pretrained vgg model\n",
        "    helper.maybe_download_pretrained_vgg(data_dir)\n",
        "\n",
        "    # OPTIONAL: Train and Inference on the cityscapes dataset instead of the Kitti dataset.\n",
        "    # You'll need a GPU with at least 10 teraFLOPS to train on.\n",
        "    #  https://www.cityscapes-dataset.com/\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        # Path to vgg model\n",
        "        vgg_path = os.path.join(data_dir, 'vgg')\n",
        "        # Create function to get batches\n",
        "        get_batches_fn = helper.gen_batch_function(os.path.join(data_dir, 'data_road/training'), image_shape)\n",
        "\n",
        "        # TODO: Augment Images for better results\n",
        "        #  https://datascience.stackexchange.com/questions/5224/how-to-prepare-augment-images-for-neural-network\n",
        "\n",
        "        # DONE: Build NN using load_vgg, layers, and optimize function\n",
        "        # Define placeholders\n",
        "        correct_label = tf.placeholder(dtype=tf.float32, shape=[None,None,None,num_classes])\n",
        "        learning_rate = tf.placeholder(dtype=tf.float32)\n",
        "\n",
        "        # Load pre-trained VGG model\n",
        "        input_image, keep_prob, vgg_layer3_out, vgg_layer4_out, vgg_layer7_out = load_vgg(sess, vgg_path)\n",
        "        \n",
        "        # Build skp-layers using VGG layers\n",
        "        nn_last_layer = layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes)\n",
        "\n",
        "        logits, train_op, cross_entropy_loss = optimize(nn_last_layer, correct_label, learning_rate, num_classes)\n",
        "\n",
        "        # DONE: Train NN using the train_nn function\n",
        "        # Initialize variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # Set training parameters\n",
        "        epochs = 20\n",
        "        batch_size = 8\n",
        "\n",
        "        train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image, \n",
        "            correct_label, keep_prob, learning_rate)\n",
        "\n",
        "        # DONE: Save inference data using helper.save_inference_samples\n",
        "        helper.save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image)\n",
        "\n",
        "        # TODO: Apply the trained model to a video\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n",
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2og9ObqGt4i_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run the model"
      ]
    },
    {
      "metadata": {
        "id": "Oq9ylZwNx0-8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SY56BTcFUiQG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---"
      ]
    }
  ]
}