{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kitti-semantic-segmentation.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": [
        "QGAjIIN2cQsu",
        "0CFLi9VCyfkO",
        "C2b2-0f-cUbY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "7Wfid0gtSPH0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Semantic Segmentation\n",
        "\n",
        "https://github.com/kcbighuge/SDCND-Semantic-Segmentation\n"
      ]
    },
    {
      "metadata": {
        "id": "eNK9L9PaTGK8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download files"
      ]
    },
    {
      "metadata": {
        "id": "gGjE84DPSgHD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P4HroAs0SZH9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 4
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ece1550d-c118-4ceb-80a0-147dd75bcf7e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521039695699,
          "user_tz": 420,
          "elapsed": 1169,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kcbighuge/SDCND-Semantic-Segmentation.git\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SDCND-Semantic-Segmentation'...\n",
            "remote: Counting objects: 48, done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 48 (delta 19), reused 48 (delta 19), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (48/48), done.\n",
            "datalab  SDCND-Semantic-Segmentation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BhtTyCLtsJGx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a761d17-ca05-4ac0-86c6-34489ab8ba82",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521047181645,
          "user_tz": 420,
          "elapsed": 488,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "\n",
        "os.chdir('SDCND-Semantic-Segmentation')\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  helper.py  LICENSE  main.py  project_tests.py  __pycache__  README.md\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NoAclbMNR6h6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aec8af56-ff1d-4a4b-993e-daa2464057fc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521039784559,
          "user_tz": 420,
          "elapsed": 80430,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "urlretrieve(\"http://kitti.is.tue.mpg.de/kitti/data_road.zip\",\"./data_road.zip\")\n",
        "with ZipFile(\"./data_road.zip\") as zipf:\n",
        "  zipf.extractall(\"./data/\")\n",
        "\n",
        "!rm data_road.zip\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  helper.py  LICENSE  main.py  project_tests.py  README.md\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QGAjIIN2cQsu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install libraries\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PSCZwqjFr7Zn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 30
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "7be52ace-2f7d-4a44-c03c-d3a3331c8da6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521047247097,
          "user_tz": 420,
          "elapsed": 44596,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "!pip install --upgrade tensorflow-gpu==1.4"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages\n",
            "Collecting tensorflow-gpu==1.4\n",
            "  Downloading tensorflow_gpu-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (170.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 170.3MB 7.5kB/s \n",
            "\u001b[?25hRequirement already up-to-date: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.4)\n",
            "Collecting numpy>=1.12.1 (from tensorflow-gpu==1.4)\n",
            "  Downloading numpy-1.14.2-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.2MB 106kB/s \n",
            "\u001b[?25hCollecting enum34>=1.1.6 (from tensorflow-gpu==1.4)\n",
            "  Downloading enum34-1.1.6-py3-none-any.whl\n",
            "Requirement already up-to-date: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.4)\n",
            "Collecting tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow-gpu==1.4)\n",
            "  Downloading tensorflow_tensorboard-0.4.0-py3-none-any.whl (1.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.7MB 710kB/s \n",
            "\u001b[?25hRequirement already up-to-date: protobuf>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.4)\n",
            "Requirement already up-to-date: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow-gpu==1.4)\n",
            "Requirement already up-to-date: bleach==1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow-gpu==1.4)\n",
            "Requirement already up-to-date: html5lib==0.9999999 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow-gpu==1.4)\n",
            "Requirement already up-to-date: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow-gpu==1.4)\n",
            "Collecting setuptools (from protobuf>=3.3.0->tensorflow-gpu==1.4)\n",
            "  Downloading setuptools-38.5.2-py2.py3-none-any.whl (490kB)\n",
            "\u001b[K    100% |████████████████████████████████| 491kB 2.2MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy, enum34, tensorflow-tensorboard, tensorflow-gpu, setuptools\n",
            "  Found existing installation: numpy 1.14.1\n",
            "    Uninstalling numpy-1.14.1:\n",
            "      Successfully uninstalled numpy-1.14.1\n",
            "  Found existing installation: setuptools 36.2.7\n",
            "    Not uninstalling setuptools at /usr/lib/python3/dist-packages, outside environment /usr\n",
            "Successfully installed enum34-1.1.6 numpy-1.14.2 setuptools-38.5.2 tensorflow-gpu-1.4.0 tensorflow-tensorboard-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0CFLi9VCyfkO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Upload files from desktop if necessary\n",
        "Make changes to training regime with `main.py` file. "
      ]
    },
    {
      "metadata": {
        "id": "9FCkBwy5VQqs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "564c4cd4-e5a9-4b56-c2c6-4cd75eb82566",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521044855302,
          "user_tz": 420,
          "elapsed": 843,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#os.rename('main (1).py', 'main.py')  # change name of dupe file\n",
        "!rm main.py\n",
        "!ls"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  helper.py  LICENSE  project_tests.py  __pycache__  README.md\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jOEgRtj7yeC9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ],
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "2f70a0e8-8301-46c6-8464-c3ea48556133",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521044893054,
          "user_tz": 420,
          "elapsed": 6139,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Working with files\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bcad79ac-2aa1-44f2-84a0-8ceeaae3356d\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-bcad79ac-2aa1-44f2-84a0-8ceeaae3356d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving main.py to main.py\n",
            "User uploaded file \"main.py\" with length 8797 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KS3P8q-qzlgg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbf1900d-f1c5-4d6c-af25-6035571846c6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521047163482,
          "user_tz": 420,
          "elapsed": 518,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datalab  SDCND-Semantic-Segmentation\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dQ-rVhIyxFl9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run the project\n",
        "Use the following command to run the project\n",
        "```\n",
        "python main.py\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "kJV4mXTTxHJQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 33
            },
            {
              "item_id": 108
            },
            {
              "item_id": 182
            },
            {
              "item_id": 209
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 7313
        },
        "outputId": "73f1e425-46a1-4b7a-dbee-29fbcf88b3b3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521049853457,
          "user_tz": 420,
          "elapsed": 1231579,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n",
            "  return f(*args, **kwds)\n",
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "TensorFlow Version: 1.4.0\n",
            "2018-03-14 17:30:23.613777: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "2018-03-14 17:30:23.738222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2018-03-14 17:30:23.738716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.11GiB\n",
            "2018-03-14 17:30:23.738743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2018-03-14 17:30:23.918828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Default GPU Device: /device:GPU:0\n",
            "2018-03-14 17:30:23.920100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Tests Passed\n",
            "Tests Passed\n",
            "2018-03-14 17:30:24.572692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Tests Passed\n",
            "2018-03-14 17:30:24.828034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Tests Passed\n",
            "Tests Passed\n",
            "2018-03-14 17:30:24.836971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if issubdtype(ts, int):\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  elif issubdtype(type(size), float):\n",
            "Epoch 1/20. Training loss: 0.6965\n",
            "\n",
            "Epoch 1/20. Training loss: 0.6919\n",
            "\n",
            "Epoch 1/20. Training loss: 0.6901\n",
            "\n",
            "Epoch 1/20. Training loss: 0.6871\n",
            "\n",
            "Epoch 1/20. Training loss: 0.6819\n",
            "\n",
            "Epoch 1/20. Training loss: 0.6762\n",
            "\n",
            "Epoch 1/20. Training loss: 0.6693\n",
            "\n",
            "Epoch 1/20. Training loss: 0.6632\n",
            "\n",
            "Epoch 1/20. Training loss: 0.6509\n",
            "\n",
            "Epoch 1/20. Training loss: 0.6493\n",
            "\n",
            "Epoch 2/20. Training loss: 0.6441\n",
            "\n",
            "Epoch 2/20. Training loss: 0.6311\n",
            "\n",
            "Epoch 2/20. Training loss: 0.6155\n",
            "\n",
            "Epoch 2/20. Training loss: 0.6009\n",
            "\n",
            "Epoch 2/20. Training loss: 0.5936\n",
            "\n",
            "Epoch 2/20. Training loss: 0.5686\n",
            "\n",
            "Epoch 2/20. Training loss: 0.5548\n",
            "\n",
            "Epoch 2/20. Training loss: 0.5343\n",
            "\n",
            "Epoch 2/20. Training loss: 0.5202\n",
            "\n",
            "Epoch 2/20. Training loss: 0.5128\n",
            "\n",
            "Epoch 3/20. Training loss: 0.4877\n",
            "\n",
            "Epoch 3/20. Training loss: 0.4802\n",
            "\n",
            "Epoch 3/20. Training loss: 0.4454\n",
            "\n",
            "Epoch 3/20. Training loss: 0.4384\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/20. Training loss: 0.4253\r\n",
            "\n",
            "Epoch 3/20. Training loss: 0.4062\n",
            "\n",
            "Epoch 3/20. Training loss: 0.4011\n",
            "\n",
            "Epoch 3/20. Training loss: 0.4027\n",
            "\n",
            "Epoch 3/20. Training loss: 0.3553\n",
            "\n",
            "Epoch 3/20. Training loss: 0.3819\n",
            "\n",
            "Epoch 4/20. Training loss: 0.3153\n",
            "\n",
            "Epoch 4/20. Training loss: 0.3267\n",
            "\n",
            "Epoch 4/20. Training loss: 0.3180\n",
            "\n",
            "Epoch 4/20. Training loss: 0.2899\n",
            "\n",
            "Epoch 4/20. Training loss: 0.2783\n",
            "\n",
            "Epoch 4/20. Training loss: 0.2572\n",
            "\n",
            "Epoch 4/20. Training loss: 0.2307\n",
            "\n",
            "Epoch 4/20. Training loss: 0.2557\n",
            "\n",
            "Epoch 4/20. Training loss: 0.2336\n",
            "\n",
            "Epoch 4/20. Training loss: 0.3605\n",
            "\n",
            "Epoch 5/20. Training loss: 0.2216\n",
            "\n",
            "Epoch 5/20. Training loss: 0.2092\n",
            "\n",
            "Epoch 5/20. Training loss: 0.2090\n",
            "\n",
            "Epoch 5/20. Training loss: 0.1869\n",
            "\n",
            "Epoch 5/20. Training loss: 0.1850\n",
            "\n",
            "Epoch 5/20. Training loss: 0.1894\n",
            "\n",
            "Epoch 5/20. Training loss: 0.1874\n",
            "\n",
            "Epoch 5/20. Training loss: 0.2147\n",
            "\n",
            "Epoch 5/20. Training loss: 0.1617\n",
            "\n",
            "Epoch 5/20. Training loss: 0.2375\n",
            "\n",
            "Epoch 6/20. Training loss: 0.1899\n",
            "\n",
            "Epoch 6/20. Training loss: 0.1911\n",
            "\n",
            "Epoch 6/20. Training loss: 0.1430\n",
            "\n",
            "Epoch 6/20. Training loss: 0.1422\n",
            "\n",
            "Epoch 6/20. Training loss: 0.1532\n",
            "\n",
            "Epoch 6/20. Training loss: 0.1645\n",
            "\n",
            "Epoch 6/20. Training loss: 0.1226\n",
            "\n",
            "Epoch 6/20. Training loss: 0.1379\n",
            "\n",
            "Epoch 6/20. Training loss: 0.1266\n",
            "\n",
            "Epoch 6/20. Training loss: 0.1567\n",
            "\n",
            "Epoch 7/20. Training loss: 0.1165\n",
            "\n",
            "Epoch 7/20. Training loss: 0.1141\n",
            "\n",
            "Epoch 7/20. Training loss: 0.1630\n",
            "\n",
            "Epoch 7/20. Training loss: 0.1220\n",
            "\n",
            "Epoch 7/20. Training loss: 0.1365\n",
            "\n",
            "Epoch 7/20. Training loss: 0.1110\n",
            "\n",
            "Epoch 7/20. Training loss: 0.1313\n",
            "\n",
            "Epoch 7/20. Training loss: 0.1003\n",
            "\n",
            "Epoch 7/20. Training loss: 0.1139\n",
            "\n",
            "Epoch 7/20. Training loss: 0.1079\n",
            "\n",
            "Epoch 8/20. Training loss: 0.0912\n",
            "\n",
            "Epoch 8/20. Training loss: 0.0999\n",
            "\n",
            "Epoch 8/20. Training loss: 0.1100\n",
            "\n",
            "Epoch 8/20. Training loss: 0.0992\n",
            "\n",
            "Epoch 8/20. Training loss: 0.1163\n",
            "\n",
            "Epoch 8/20. Training loss: 0.1478\n",
            "\n",
            "Epoch 8/20. Training loss: 0.1179\n",
            "\n",
            "Epoch 8/20. Training loss: 0.0880\n",
            "\n",
            "Epoch 8/20. Training loss: 0.1019\n",
            "\n",
            "Epoch 8/20. Training loss: 0.0754\n",
            "\n",
            "Epoch 9/20. Training loss: 0.1005\n",
            "\n",
            "Epoch 9/20. Training loss: 0.0999\n",
            "\n",
            "Epoch 9/20. Training loss: 0.1060\n",
            "\n",
            "Epoch 9/20. Training loss: 0.0943\n",
            "\n",
            "Epoch 9/20. Training loss: 0.0868\n",
            "\n",
            "Epoch 9/20. Training loss: 0.0904\n",
            "\n",
            "Epoch 9/20. Training loss: 0.1096\n",
            "\n",
            "Epoch 9/20. Training loss: 0.1083\n",
            "\n",
            "Epoch 9/20. Training loss: 0.0924\n",
            "\n",
            "Epoch 9/20. Training loss: 0.2376\n",
            "\n",
            "Epoch 10/20. Training loss: 0.1196\n",
            "\n",
            "Epoch 10/20. Training loss: 0.1258\n",
            "\n",
            "Epoch 10/20. Training loss: 0.1117\n",
            "\n",
            "Epoch 10/20. Training loss: 0.1278\n",
            "\n",
            "Epoch 10/20. Training loss: 0.1280\n",
            "\n",
            "Epoch 10/20. Training loss: 0.1198\n",
            "\n",
            "Epoch 10/20. Training loss: 0.0975\n",
            "\n",
            "Epoch 10/20. Training loss: 0.0998\n",
            "\n",
            "Epoch 10/20. Training loss: 0.0816\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/20. Training loss: 0.0508\r\n",
            "\n",
            "Epoch 11/20. Training loss: 0.0865\n",
            "\n",
            "Epoch 11/20. Training loss: 0.0609\n",
            "\n",
            "Epoch 11/20. Training loss: 0.1069\n",
            "\n",
            "Epoch 11/20. Training loss: 0.0754\n",
            "\n",
            "Epoch 11/20. Training loss: 0.0744\n",
            "\n",
            "Epoch 11/20. Training loss: 0.0848\n",
            "\n",
            "Epoch 11/20. Training loss: 0.0740\n",
            "\n",
            "Epoch 11/20. Training loss: 0.0903\n",
            "\n",
            "Epoch 11/20. Training loss: 0.0895\n",
            "\n",
            "Epoch 11/20. Training loss: 0.0888\n",
            "\n",
            "Epoch 12/20. Training loss: 0.0883\n",
            "\n",
            "Epoch 12/20. Training loss: 0.0676\n",
            "\n",
            "Epoch 12/20. Training loss: 0.1172\n",
            "\n",
            "Epoch 12/20. Training loss: 0.0759\n",
            "\n",
            "Epoch 12/20. Training loss: 0.0948\n",
            "\n",
            "Epoch 12/20. Training loss: 0.0963\n",
            "\n",
            "Epoch 12/20. Training loss: 0.0832\n",
            "\n",
            "Epoch 12/20. Training loss: 0.0594\n",
            "\n",
            "Epoch 12/20. Training loss: 0.0670\n",
            "\n",
            "Epoch 12/20. Training loss: 0.0674\n",
            "\n",
            "Epoch 13/20. Training loss: 0.0729\n",
            "\n",
            "Epoch 13/20. Training loss: 0.0805\n",
            "\n",
            "Epoch 13/20. Training loss: 0.0749\n",
            "\n",
            "Epoch 13/20. Training loss: 0.0880\n",
            "\n",
            "Epoch 13/20. Training loss: 0.0602\n",
            "\n",
            "Epoch 13/20. Training loss: 0.0746\n",
            "\n",
            "Epoch 13/20. Training loss: 0.0603\n",
            "\n",
            "Epoch 13/20. Training loss: 0.0858\n",
            "\n",
            "Epoch 13/20. Training loss: 0.0717\n",
            "\n",
            "Epoch 13/20. Training loss: 0.1984\n",
            "\n",
            "Epoch 14/20. Training loss: 0.0816\n",
            "\n",
            "Epoch 14/20. Training loss: 0.0676\n",
            "\n",
            "Epoch 14/20. Training loss: 0.0520\n",
            "\n",
            "Epoch 14/20. Training loss: 0.0664\n",
            "\n",
            "Epoch 14/20. Training loss: 0.0757\n",
            "\n",
            "Epoch 14/20. Training loss: 0.0775\n",
            "\n",
            "Epoch 14/20. Training loss: 0.0603\n",
            "\n",
            "Epoch 14/20. Training loss: 0.0686\n",
            "\n",
            "Epoch 14/20. Training loss: 0.0836\n",
            "\n",
            "Epoch 14/20. Training loss: 0.0635\n",
            "\n",
            "Epoch 15/20. Training loss: 0.0484\n",
            "\n",
            "Epoch 15/20. Training loss: 0.0767\n",
            "\n",
            "Epoch 15/20. Training loss: 0.0654\n",
            "\n",
            "Epoch 15/20. Training loss: 0.0604\n",
            "\n",
            "Epoch 15/20. Training loss: 0.0625\n",
            "\n",
            "Epoch 15/20. Training loss: 0.0714\n",
            "\n",
            "Epoch 15/20. Training loss: 0.0533\n",
            "\n",
            "Epoch 15/20. Training loss: 0.0450\n",
            "\n",
            "Epoch 15/20. Training loss: 0.0556\n",
            "\n",
            "Epoch 15/20. Training loss: 0.0455\n",
            "\n",
            "Epoch 16/20. Training loss: 0.0554\n",
            "\n",
            "Epoch 16/20. Training loss: 0.0430\n",
            "\n",
            "Epoch 16/20. Training loss: 0.0582\n",
            "\n",
            "Epoch 16/20. Training loss: 0.0631\n",
            "\n",
            "Epoch 16/20. Training loss: 0.0514\n",
            "\n",
            "Epoch 16/20. Training loss: 0.0428\n",
            "\n",
            "Epoch 16/20. Training loss: 0.0508\n",
            "\n",
            "Epoch 16/20. Training loss: 0.0658\n",
            "\n",
            "Epoch 16/20. Training loss: 0.0632\n",
            "\n",
            "Epoch 16/20. Training loss: 0.0738\n",
            "\n",
            "Epoch 17/20. Training loss: 0.0548\n",
            "\n",
            "Epoch 17/20. Training loss: 0.0614\n",
            "\n",
            "Epoch 17/20. Training loss: 0.0578\n",
            "\n",
            "Epoch 17/20. Training loss: 0.0573\n",
            "\n",
            "Epoch 17/20. Training loss: 0.0380\n",
            "\n",
            "Epoch 17/20. Training loss: 0.0546\n",
            "\n",
            "Epoch 17/20. Training loss: 0.0486\n",
            "\n",
            "Epoch 17/20. Training loss: 0.0659\n",
            "\n",
            "Epoch 17/20. Training loss: 0.0589\n",
            "\n",
            "Epoch 17/20. Training loss: 0.0265\n",
            "\n",
            "Epoch 18/20. Training loss: 0.0408\n",
            "\n",
            "Epoch 18/20. Training loss: 0.0585\n",
            "\n",
            "Epoch 18/20. Training loss: 0.0586\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/20. Training loss: 0.0611\r\n",
            "\n",
            "Epoch 18/20. Training loss: 0.0505\n",
            "\n",
            "Epoch 18/20. Training loss: 0.0575\n",
            "\n",
            "Epoch 18/20. Training loss: 0.0415\n",
            "\n",
            "Epoch 18/20. Training loss: 0.0471\n",
            "\n",
            "Epoch 18/20. Training loss: 0.0585\n",
            "\n",
            "Epoch 18/20. Training loss: 0.0295\n",
            "\n",
            "Epoch 19/20. Training loss: 0.0428\n",
            "\n",
            "Epoch 19/20. Training loss: 0.0459\n",
            "\n",
            "Epoch 19/20. Training loss: 0.0468\n",
            "\n",
            "Epoch 19/20. Training loss: 0.0492\n",
            "\n",
            "Epoch 19/20. Training loss: 0.0533\n",
            "\n",
            "Epoch 19/20. Training loss: 0.0466\n",
            "\n",
            "Epoch 19/20. Training loss: 0.0460\n",
            "\n",
            "Epoch 19/20. Training loss: 0.0431\n",
            "\n",
            "Epoch 19/20. Training loss: 0.0478\n",
            "\n",
            "Epoch 19/20. Training loss: 0.0400\n",
            "\n",
            "Epoch 20/20. Training loss: 0.0509\n",
            "\n",
            "Epoch 20/20. Training loss: 0.0561\n",
            "\n",
            "Epoch 20/20. Training loss: 0.0462\n",
            "\n",
            "Epoch 20/20. Training loss: 0.0507\n",
            "\n",
            "Epoch 20/20. Training loss: 0.0436\n",
            "\n",
            "Epoch 20/20. Training loss: 0.0592\n",
            "\n",
            "Epoch 20/20. Training loss: 0.0506\n",
            "\n",
            "Epoch 20/20. Training loss: 0.0489\n",
            "\n",
            "Epoch 20/20. Training loss: 0.0535\n",
            "\n",
            "Epoch 20/20. Training loss: 0.0567\n",
            "\n",
            "Training Finished. Saving test images to: ./runs/1521049777.2365575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w2yyEy4P1YF_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download files"
      ]
    },
    {
      "metadata": {
        "id": "EuhpG6FJ6h08",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "89f8b651-ff04-4e17-c29b-f0cd97ad48da",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521050079811,
          "user_tz": 420,
          "elapsed": 2133,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive('runs_img', 'zip', './runs/1521049777.2365575/')\n",
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t   LICENSE  project_tests.py  README.md  runs_img.zip\r\n",
            "helper.py  main.py  __pycache__       runs\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0zFh4vTY5HVY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('runs_img.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C2b2-0f-cUbY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building the model"
      ]
    },
    {
      "metadata": {
        "id": "aNOWaSdIcPnW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dc6fa277-55a9-476f-9f5e-3a8c6e1774dd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521039895106,
          "user_tz": 420,
          "elapsed": 7700,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import tensorflow as tf\n",
        "import helper\n",
        "import warnings\n",
        "from distutils.version import LooseVersion\n",
        "import project_tests as tests\n",
        "\n",
        "\n",
        "# Check TensorFlow Version\n",
        "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))\n",
        "\n",
        "# Check for a GPU\n",
        "if not tf.test.gpu_device_name():\n",
        "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
        "else:\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 1.6.0\n",
            "Default GPU Device: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "atPM4MiccaCQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "510e2cf1-f2c9-49fd-d239-a2bd47ee3c69",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521044682552,
          "user_tz": 420,
          "elapsed": 554,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def load_vgg(sess, vgg_path):\n",
        "    \"\"\"\n",
        "    Load Pretrained VGG Model into TensorFlow.\n",
        "    :param sess: TensorFlow Session\n",
        "    :param vgg_path: Path to vgg folder, containing \"variables/\" and \"saved_model.pb\"\n",
        "    :return: Tuple of Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)\n",
        "    \"\"\"\n",
        "    # DONE: Implement function\n",
        "    #  Use tf.saved_model.loader.load to load the model and weights\n",
        "    vgg_tag = 'vgg16'\n",
        "    vgg_input_tensor_name = 'image_input:0'\n",
        "    vgg_keep_prob_tensor_name = 'keep_prob:0'\n",
        "    vgg_layer3_out_tensor_name = 'layer3_out:0'\n",
        "    vgg_layer4_out_tensor_name = 'layer4_out:0'\n",
        "    vgg_layer7_out_tensor_name = 'layer7_out:0'\n",
        "\n",
        "    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)\n",
        "    graph = tf.get_default_graph()\n",
        "    \n",
        "    image_input = graph.get_tensor_by_name(vgg_input_tensor_name)\n",
        "    keep_prob = graph.get_tensor_by_name(vgg_keep_prob_tensor_name)\n",
        "\n",
        "    layer3_out = graph.get_tensor_by_name(vgg_layer3_out_tensor_name)\n",
        "\n",
        "    layer4_out = graph.get_tensor_by_name(vgg_layer4_out_tensor_name)\n",
        "\n",
        "    layer7_out = graph.get_tensor_by_name(vgg_layer7_out_tensor_name)\n",
        "\n",
        "    return image_input, keep_prob, layer3_out, layer4_out, layer7_out\n",
        "\n",
        "tests.test_load_vgg(load_vgg, tf)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5ud-1ZI1cfPJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61f3cc8e-0423-4ba5-cc5d-c665eab3eb93",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521044706403,
          "user_tz": 420,
          "elapsed": 1417,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n",
        "    \"\"\"\n",
        "    Create the layers for a fully convolutional network.  Build skip-layers using the vgg layers.\n",
        "    :param vgg_layer3_out: TF Tensor for VGG Layer 3 output\n",
        "    :param vgg_layer4_out: TF Tensor for VGG Layer 4 output\n",
        "    :param vgg_layer7_out: TF Tensor for VGG Layer 7 output\n",
        "    :param num_classes: Number of classes to classify\n",
        "    :return: The Tensor for the last layer of output\n",
        "    \"\"\"\n",
        "    # DONE: Implement function\n",
        "    layer7_1x1 = tf.layers.conv2d(vgg_layer7_out, num_classes, 1, \n",
        "                                  padding='same', \n",
        "                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n",
        "    \n",
        "    output = tf.layers.conv2d_transpose(layer7_1x1, num_classes, 4, \n",
        "                                        strides=(2,2), padding='same', \n",
        "                                        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n",
        "    \n",
        "    # scale output of layers 3 & 4 before feeding to 1x1 convolution\n",
        "    pool3_out_scaled = tf.multiply(vgg_layer3_out, 0.0001, name='pool3_out_scaled')\n",
        "    pool4_out_scaled = tf.multiply(vgg_layer4_out, 0.01, name='pool4_out_scaled')\n",
        "\n",
        "    layer4_1x1 = tf.layers.conv2d(pool4_out_scaled, num_classes, 1, \n",
        "                                  padding='same', \n",
        "                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n",
        "    \n",
        "    output = tf.add(output, layer4_1x1)\n",
        "    output = tf.layers.conv2d_transpose(output, num_classes, 4, \n",
        "                                        strides=(2,2), \n",
        "                                        padding='same', \n",
        "                                        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n",
        "    \n",
        "    layer3_1x1 = tf.layers.conv2d(pool3_out_scaled, num_classes, 1, \n",
        "                                  padding='same', \n",
        "                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n",
        "    \n",
        "    output = tf.add(output, layer3_1x1)\n",
        "    output = tf.layers.conv2d_transpose(output, num_classes, 16, \n",
        "                                        strides=(8,8), padding='same', \n",
        "                                        kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n",
        "\n",
        "    return output\n",
        "\n",
        "tests.test_layers(layers)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TUJDLPbchbjA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14e8db4b-e163-4db1-c901-60103a108b41",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521017741826,
          "user_tz": 420,
          "elapsed": 593,
          "user": {
            "displayName": "James Lee",
            "photoUrl": "//lh4.googleusercontent.com/--1yZ2JZEuV4/AAAAAAAAAAI/AAAAAAAAH2w/T_-2EYsROis/s50-c-k-no/photo.jpg",
            "userId": "107530914765411970840"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def optimize(nn_last_layer, correct_label, learning_rate, num_classes):\n",
        "    \"\"\"\n",
        "    Build the TensorFLow loss and optimizer operations.\n",
        "    :param nn_last_layer: TF Tensor of the last layer in the neural network\n",
        "    :param correct_label: TF Placeholder for the correct label image\n",
        "    :param learning_rate: TF Placeholder for the learning rate\n",
        "    :param num_classes: Number of classes to classify\n",
        "    :return: Tuple of (logits, train_op, cross_entropy_loss)\n",
        "    \"\"\"\n",
        "    # DONE: Implement function\n",
        "    # Convert 4D output tensor to 2D with shape (num pixels, num classes)\n",
        "    logits = tf.reshape(nn_last_layer, (-1, num_classes))\n",
        "    labels = tf.reshape(correct_label, (-1, num_classes))\n",
        "\n",
        "    # Define loss\n",
        "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
        "    cross_entropy_loss = tf.reduce_mean(cross_entropy)\n",
        "\n",
        "    # Define optimization\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy_loss)\n",
        "\n",
        "    return logits, train_op, cross_entropy_loss\n",
        "\n",
        "tests.test_optimize(optimize)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ufiU5pkdcqg6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image,\n",
        "             correct_label, keep_prob, learning_rate):\n",
        "    \"\"\"\n",
        "    Train neural network and print out the loss during training.\n",
        "    :param sess: TF Session\n",
        "    :param epochs: Number of epochs\n",
        "    :param batch_size: Batch size\n",
        "    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n",
        "    :param train_op: TF Operation to train the neural network\n",
        "    :param cross_entropy_loss: TF Tensor for the amount of loss\n",
        "    :param input_image: TF Placeholder for input images\n",
        "    :param correct_label: TF Placeholder for label images\n",
        "    :param keep_prob: TF Placeholder for dropout keep probability\n",
        "    :param learning_rate: TF Placeholder for learning rate\n",
        "    \"\"\"\n",
        "    # DONE: Implement function\n",
        "    for e in range(epochs):\n",
        "\n",
        "        for images, labels in get_batches_fn(batch_size):\n",
        "            _, loss = sess.run([train_op, cross_entropy_loss], \n",
        "                               feed_dict={input_image: images, \n",
        "                                          correct_label: labels, \n",
        "                                          keep_prob: 0.5, \n",
        "                                          learning_rate: 1e-4})\n",
        "\n",
        "            print(\"Epoch {}/{}. Training loss: {:.4f}\\n\".format(e+1, epochs, loss))\n",
        "\n",
        "tests.test_train_nn(train_nn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9NWG04n5cnXC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def run():\n",
        "    num_classes = 2\n",
        "    image_shape = (160, 576)\n",
        "    # Download data: http://kitti.is.tue.mpg.de/kitti/data_road.zip\n",
        "    data_dir = './data'\n",
        "    runs_dir = './runs'\n",
        "    tests.test_for_kitti_dataset(data_dir)\n",
        "\n",
        "    # Download pretrained vgg model\n",
        "    helper.maybe_download_pretrained_vgg(data_dir)\n",
        "\n",
        "    # OPTIONAL: Train and Inference on the cityscapes dataset instead of the Kitti dataset.\n",
        "    # You'll need a GPU with at least 10 teraFLOPS to train on.\n",
        "    #  https://www.cityscapes-dataset.com/\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        # Path to vgg model\n",
        "        vgg_path = os.path.join(data_dir, 'vgg')\n",
        "        # Create function to get batches\n",
        "        get_batches_fn = helper.gen_batch_function(os.path.join(data_dir, 'data_road/training'), image_shape)\n",
        "\n",
        "        # TODO: Augment Images for better results\n",
        "        #  https://datascience.stackexchange.com/questions/5224/how-to-prepare-augment-images-for-neural-network\n",
        "\n",
        "        # DONE: Build NN using load_vgg, layers, and optimize function\n",
        "        # Define placeholders\n",
        "        correct_label = tf.placeholder(dtype=tf.float32, shape=[None,None,None,num_classes])\n",
        "        learning_rate = tf.placeholder(dtype=tf.float32)\n",
        "\n",
        "        # Load pre-trained VGG model\n",
        "        input_image, keep_prob, vgg_layer3_out, vgg_layer4_out, vgg_layer7_out = load_vgg(sess, vgg_path)\n",
        "        \n",
        "        # Build skp-layers using VGG layers\n",
        "        nn_last_layer = layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes)\n",
        "\n",
        "        logits, train_op, cross_entropy_loss = optimize(nn_last_layer, correct_label, learning_rate, num_classes)\n",
        "\n",
        "        # DONE: Train NN using the train_nn function\n",
        "        # Initialize variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # Set training parameters\n",
        "        epochs = 40\n",
        "        batch_size = 8\n",
        "\n",
        "        train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image, \n",
        "            correct_label, keep_prob, learning_rate)\n",
        "\n",
        "        # DONE: Save inference data using helper.save_inference_samples\n",
        "        helper.save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image)\n",
        "\n",
        "        # TODO: Apply the trained model to a video\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2og9ObqGt4i_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run the model"
      ]
    },
    {
      "metadata": {
        "id": "Oq9ylZwNx0-8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SY56BTcFUiQG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---"
      ]
    }
  ]
}